{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOMEIM8iW/RI5hoGSskmagw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/carlm451/Gemstone_Images_Classification_Fine_Tuning/blob/main/Gemstones_Classifier_MobileNet_V1_Tests.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "ztziw_8J_Vva"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from random import shuffle\n",
        "\n",
        "from tensorflow.keras.applications import MobileNet # V1 mobile net version "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download/Prepare Kaggle Gemstones Dataset\n",
        "\n",
        "need to have an API key from kaggle, follow instructions here https://www.kaggle.com/docs/api"
      ],
      "metadata": {
        "id": "5MaKbAEI_3QJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "NK-3nuGlHu3q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6aee36f4-6f9d-4354-af30-c4728998e66b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading gemstones-images.zip to /content\n",
            " 91% 50.0M/55.2M [00:01<00:00, 50.6MB/s]\n",
            "100% 55.2M/55.2M [00:01<00:00, 51.4MB/s]\n",
            "gemstones-images.zip  sample_data  test  train\n"
          ]
        }
      ],
      "source": [
        "# pulling gemstones data from kaggle\n",
        "#!pip install kaggle\n",
        "\n",
        "!mkdir ~/.kaggle\n",
        "\n",
        "#need a kaggle API key kaggle.json\n",
        "# I just upload it from my pc to /content colab directory \n",
        "!mv kaggle.json ~/.kaggle\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "#download the images to local colab drive\n",
        "!kaggle datasets download lsind18/gemstones-images\n",
        "\n",
        "# unzip the images\n",
        "!unzip gemstones-images.zip &> /dev/null  #suppress terminal output when unzipping images\n",
        "\n",
        "#expect to zee gemstones-images.zip, and test and train directories \n",
        "!ls "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "data_dir = '/content'\n",
        "\n",
        "train_dir = os.path.join(data_dir,'train')\n",
        "\n",
        "!mkdir val # going to use some training data for validation, save test data for final model evaluation \n",
        "val_dir = os.path.join(data_dir,'val')\n",
        "\n",
        "test_dir = os.path.join(data_dir,'test')\n",
        "\n",
        "def count_img_samples(directory):\n",
        "    \n",
        "    count = 0\n",
        "    \n",
        "    for i,gem_type in enumerate(os.listdir(directory)):\n",
        "        \n",
        "        gem_dir = os.path.join(directory,gem_type)\n",
        "    \n",
        "        img_list = os.listdir(gem_dir)\n",
        "\n",
        "        #print(f' dir {gem_dir} has {len(img_list)} images')\n",
        "\n",
        "        count += len(img_list)\n",
        "    \n",
        "    return count\n",
        "\n",
        "n_train = count_img_samples(train_dir)\n",
        "n_test = count_img_samples(test_dir)\n",
        "n_val = count_img_samples(val_dir)\n",
        "\n",
        "print(f'{n_train=}, {n_val=}, {n_test=}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pp-hFLr7JSjf",
        "outputId": "bcbb8ec6-4726-4ce9-8393-fd97a19aaf5d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n_train=2856, n_val=0, n_test=363\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from random import shuffle\n",
        "\n",
        "def partition_val_data(train_dir,val_dir,val_split=0.1):\n",
        "    \n",
        "    for gem_type in os.listdir(train_dir):\n",
        "        \n",
        "        train_gem_dir = os.path.join(train_dir,gem_type)\n",
        "        \n",
        "        img_list = os.listdir(train_gem_dir)\n",
        "        \n",
        "        shuffle(img_list)\n",
        "        \n",
        "        n_samples = round(len(img_list)*val_split)\n",
        "        \n",
        "        val_img_list = img_list[:n_samples] # take n_samples random images to move\n",
        "        \n",
        "        val_gem_dir = os.path.join(val_dir,gem_type)\n",
        "        \n",
        "        if not os.path.exists(val_gem_dir):\n",
        "            \n",
        "            os.mkdir(val_gem_dir)\n",
        "            \n",
        "            for gem_img in val_img_list:\n",
        "                \n",
        "                original_path = os.path.join(train_gem_dir,gem_img)\n",
        "                \n",
        "                destination_path = os.path.join(val_gem_dir,gem_img)\n",
        "                \n",
        "                os.rename(original_path,destination_path)\n",
        "        \n",
        "            #print(f'Moved {len(os.listdir(val_gem_dir))} training images from to {val_gem_dir}')\n",
        "            \n",
        "        else:\n",
        "            \n",
        "            pass\n",
        "            #print(f'Val dir {val_gem_dir} has {len(os.listdir(val_gem_dir))} images')"
      ],
      "metadata": {
        "id": "Fq25nvVbJl24"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_split=0.15  # move 15% train to use for validation\n",
        "\n",
        "partition_val_data(train_dir,val_dir,val_split)\n",
        "\n",
        "n_train = count_img_samples(train_dir)\n",
        "n_test = count_img_samples(test_dir)\n",
        "n_val = count_img_samples(val_dir)\n",
        "\n",
        "print(f'{n_train=}, {n_val=}, {n_test=}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQcGtSnWJ8Ve",
        "outputId": "c86ea162-22da-4134-fac5-0175a3ff49eb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n_train=2434, n_val=422, n_test=363\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls val\n",
        "\n",
        "gem_types_list = os.listdir(val_dir)\n",
        "\n",
        "n_classes = len(gem_types_list)\n",
        "\n",
        "print(f'{n_classes} classes of gemstone')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rylLl31YCc54",
        "outputId": "0eb7095d-b10d-4022-cb86-cccc9e428dd0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Alexandrite\t      Chrysocolla     Larimar\t\t 'Sapphire Blue'\n",
            " Almandine\t      Chrysoprase     Malachite\t\t 'Sapphire Pink'\n",
            " Amazonite\t      Citrine\t      Moonstone\t\t 'Sapphire Purple'\n",
            " Amber\t\t      Coral\t      Morganite\t\t 'Sapphire Yellow'\n",
            " Amethyst\t      Danburite      'Onyx Black'\t  Scapolite\n",
            " Ametrine\t      Diamond\t     'Onyx Green'\t  Serpentine\n",
            " Andalusite\t      Diaspore\t     'Onyx Red'\t\t  Sodalite\n",
            " Andradite\t      Dumortierite    Opal\t\t  Spessartite\n",
            " Aquamarine\t      Emerald\t      Pearl\t\t  Sphene\n",
            "'Aventurine Green'    Fluorite\t      Peridot\t\t  Spinel\n",
            "'Aventurine Yellow'  'Garnet Red'     Prehnite\t\t  Spodumene\n",
            " Benitoite\t      Goshenite       Pyrite\t\t  Sunstone\n",
            "'Beryl Golden'\t      Grossular       Pyrope\t\t  Tanzanite\n",
            " Bixbite\t      Hessonite      'Quartz Beer'\t 'Tigers Eye'\n",
            " Bloodstone\t      Hiddenite      'Quartz Lemon'\t  Topaz\n",
            "'Blue Lace Agate'     Iolite\t     'Quartz Rose'\t  Tourmaline\n",
            " Carnelian\t      Jade\t     'Quartz Rutilated'   Tsavorite\n",
            "'Cats Eye'\t      Jasper\t     'Quartz Smoky'\t  Turquoise\n",
            " Chalcedony\t      Kunzite\t      Rhodochrosite\t  Variscite\n",
            "'Chalcedony Blue'     Kyanite\t      Rhodolite\t\t  Zircon\n",
            "'Chrome Diopside'     Labradorite     Rhodonite\t\t  Zoisite\n",
            " Chrysoberyl\t     'Lapis Lazuli'   Ruby\n",
            "87 classes of gemstone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# generators to stream images for training/validation\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "#from tensorflow.keras.applications.efficientnet_v2 import preprocess_input\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale = 1.0/255.,\n",
        "                                   rotation_range=90,\n",
        "                                   width_shift_range=0.4,\n",
        "                                   height_shift_range=0.4,\n",
        "                                   zoom_range=0.5,\n",
        "                                   horizontal_flip=True,\n",
        "                                   vertical_flip=True\n",
        "                                  )\n",
        "\n",
        "val_datagen  = ImageDataGenerator( rescale = 1.0/255.)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(train_dir,\n",
        "                                                    batch_size=64,\n",
        "                                                    class_mode='categorical',\n",
        "                                                    target_size=(224, 224),\n",
        "                                                    keep_aspect_ratio=False,\n",
        "                                                    classes=gem_types_list) \n",
        "\n",
        "val_generator = val_datagen.flow_from_directory(val_dir,\n",
        "                                                    batch_size=64,\n",
        "                                                    class_mode='categorical',\n",
        "                                                    target_size=(224,224),\n",
        "                                                    keep_aspect_ratio=False,\n",
        "                                                    classes=gem_types_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NPf45TDBCVFU",
        "outputId": "5bcf2095-6fcd-44dd-fad6-ecfb36d05899"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2434 images belonging to 87 classes.\n",
            "Found 422 images belonging to 87 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classifier based on pretrained MobileNet V1 model"
      ],
      "metadata": {
        "id": "eC2CNSEaCC4C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_uncompiled_model(n_classes, model_name,fine_tune=0):\n",
        "\n",
        "    tf.keras.backend.clear_session()\n",
        "\n",
        "    pretrained = MobileNet(include_top=False,pooling='avg',input_shape=(224,224,3))\n",
        "\n",
        "    if fine_tune > 0:\n",
        "        for layer in pretrained.layers[:-fine_tune]:\n",
        "            layer.trainable = False\n",
        "    else:\n",
        "        pretrained.trainable=False #freezes all children layers \n",
        "\n",
        "    inputs=tf.keras.layers.Input(shape=(224,224,3))\n",
        "\n",
        "    x=pretrained(inputs,training=False)\n",
        "\n",
        "    x = tf.keras.layers.Dense(n_classes)(x)  # make sure to use from_logits=True in loss later on\n",
        "\n",
        "    model = tf.keras.Model(inputs=inputs,outputs=x)\n",
        "    \n",
        "    return model"
      ],
      "metadata": {
        "id": "-rgQ76g_LJXL"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_1 = get_uncompiled_model(n_classes,model_name='mobileV1_frozen')\n",
        "    \n",
        "model_1.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hzSPtY86Cwrz",
        "outputId": "9b7ca483-4386-4c5d-a475-37b9fa71cb90"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet/mobilenet_1_0_224_tf_no_top.h5\n",
            "17225924/17225924 [==============================] - 0s 0us/step\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
            "                                                                 \n",
            " mobilenet_1.00_224 (Functio  (None, 1024)             3228864   \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dense (Dense)               (None, 87)                89175     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,318,039\n",
            "Trainable params: 89,175\n",
            "Non-trainable params: 3,228,864\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "base_learning_rate = 0.0005\n",
        "\n",
        "model_1.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=base_learning_rate),\n",
        "              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "              metrics = [tf.keras.metrics.CategoricalAccuracy()])\n",
        "\n",
        "!rm -r training_1\n",
        "\n",
        "!mkdir training_1"
      ],
      "metadata": {
        "id": "O2KClwCBC1LM"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scheduler(epoch, lr):\n",
        "    return lr\n",
        "\n",
        "lr_callback = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
        "\n",
        "checkpoint_path = \"training_1/cp-{epoch:04d}.ckpt\"\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "N_EPOCHS=25\n",
        "\n",
        "# Create a callback that saves the model's weights\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
        "                                                 save_weights_only=True,\n",
        "                                                 save_best_only=True,\n",
        "                                                 verbose=1,\n",
        "                                                 save_freq='epoch')\n",
        "\n",
        "history_1 = model_1.fit(\n",
        "            train_generator,\n",
        "            epochs=N_EPOCHS,\n",
        "            validation_data=val_generator,\n",
        "            verbose=1,\n",
        "            callbacks=[cp_callback,lr_callback]\n",
        "            )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1HCvfWZXDG5k",
        "outputId": "2c94535e-18bf-405d-89f6-7dc1cb6dc7eb"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "39/39 [==============================] - ETA: 0s - loss: 4.3745 - categorical_accuracy: 0.0477\n",
            "Epoch 1: val_loss improved from inf to 3.88815, saving model to training_1/cp-0001.ckpt\n",
            "39/39 [==============================] - 39s 935ms/step - loss: 4.3745 - categorical_accuracy: 0.0477 - val_loss: 3.8881 - val_categorical_accuracy: 0.0853 - lr: 5.0000e-04\n",
            "Epoch 2/25\n",
            "39/39 [==============================] - ETA: 0s - loss: 3.5287 - categorical_accuracy: 0.1845\n",
            "Epoch 2: val_loss improved from 3.88815 to 3.25181, saving model to training_1/cp-0002.ckpt\n",
            "39/39 [==============================] - 34s 873ms/step - loss: 3.5287 - categorical_accuracy: 0.1845 - val_loss: 3.2518 - val_categorical_accuracy: 0.2370 - lr: 5.0000e-04\n",
            "Epoch 3/25\n",
            "39/39 [==============================] - ETA: 0s - loss: 3.0000 - categorical_accuracy: 0.3155\n",
            "Epoch 3: val_loss improved from 3.25181 to 2.83564, saving model to training_1/cp-0003.ckpt\n",
            "39/39 [==============================] - 35s 889ms/step - loss: 3.0000 - categorical_accuracy: 0.3155 - val_loss: 2.8356 - val_categorical_accuracy: 0.3270 - lr: 5.0000e-04\n",
            "Epoch 4/25\n",
            "39/39 [==============================] - ETA: 0s - loss: 2.6528 - categorical_accuracy: 0.3821\n",
            "Epoch 4: val_loss improved from 2.83564 to 2.55935, saving model to training_1/cp-0004.ckpt\n",
            "39/39 [==============================] - 34s 901ms/step - loss: 2.6528 - categorical_accuracy: 0.3821 - val_loss: 2.5593 - val_categorical_accuracy: 0.3863 - lr: 5.0000e-04\n",
            "Epoch 5/25\n",
            "39/39 [==============================] - ETA: 0s - loss: 2.3957 - categorical_accuracy: 0.4441\n",
            "Epoch 5: val_loss improved from 2.55935 to 2.34932, saving model to training_1/cp-0005.ckpt\n",
            "39/39 [==============================] - 34s 857ms/step - loss: 2.3957 - categorical_accuracy: 0.4441 - val_loss: 2.3493 - val_categorical_accuracy: 0.4431 - lr: 5.0000e-04\n",
            "Epoch 6/25\n",
            "39/39 [==============================] - ETA: 0s - loss: 2.2250 - categorical_accuracy: 0.4651\n",
            "Epoch 6: val_loss improved from 2.34932 to 2.21932, saving model to training_1/cp-0006.ckpt\n",
            "39/39 [==============================] - 34s 877ms/step - loss: 2.2250 - categorical_accuracy: 0.4651 - val_loss: 2.2193 - val_categorical_accuracy: 0.4621 - lr: 5.0000e-04\n",
            "Epoch 7/25\n",
            "39/39 [==============================] - ETA: 0s - loss: 2.0392 - categorical_accuracy: 0.5066\n",
            "Epoch 7: val_loss improved from 2.21932 to 2.09415, saving model to training_1/cp-0007.ckpt\n",
            "39/39 [==============================] - 34s 881ms/step - loss: 2.0392 - categorical_accuracy: 0.5066 - val_loss: 2.0942 - val_categorical_accuracy: 0.4763 - lr: 5.0000e-04\n",
            "Epoch 8/25\n",
            "38/39 [============================>.] - ETA: 0s - loss: 1.9304 - categorical_accuracy: 0.5312\n",
            "Epoch 8: val_loss improved from 2.09415 to 2.00654, saving model to training_1/cp-0008.ckpt\n",
            "39/39 [==============================] - 35s 882ms/step - loss: 1.9300 - categorical_accuracy: 0.5316 - val_loss: 2.0065 - val_categorical_accuracy: 0.4929 - lr: 5.0000e-04\n",
            "Epoch 9/25\n",
            "39/39 [==============================] - ETA: 0s - loss: 1.8372 - categorical_accuracy: 0.5456\n",
            "Epoch 9: val_loss improved from 2.00654 to 1.92749, saving model to training_1/cp-0009.ckpt\n",
            "39/39 [==============================] - 34s 882ms/step - loss: 1.8372 - categorical_accuracy: 0.5456 - val_loss: 1.9275 - val_categorical_accuracy: 0.5095 - lr: 5.0000e-04\n",
            "Epoch 10/25\n",
            "39/39 [==============================] - ETA: 0s - loss: 1.7228 - categorical_accuracy: 0.5826\n",
            "Epoch 10: val_loss improved from 1.92749 to 1.84700, saving model to training_1/cp-0010.ckpt\n",
            "39/39 [==============================] - 33s 854ms/step - loss: 1.7228 - categorical_accuracy: 0.5826 - val_loss: 1.8470 - val_categorical_accuracy: 0.5142 - lr: 5.0000e-04\n",
            "Epoch 11/25\n",
            "39/39 [==============================] - ETA: 0s - loss: 1.6537 - categorical_accuracy: 0.5842\n",
            "Epoch 11: val_loss improved from 1.84700 to 1.79327, saving model to training_1/cp-0011.ckpt\n",
            "39/39 [==============================] - 34s 872ms/step - loss: 1.6537 - categorical_accuracy: 0.5842 - val_loss: 1.7933 - val_categorical_accuracy: 0.5237 - lr: 5.0000e-04\n",
            "Epoch 12/25\n",
            "39/39 [==============================] - ETA: 0s - loss: 1.6105 - categorical_accuracy: 0.5887\n",
            "Epoch 12: val_loss improved from 1.79327 to 1.75586, saving model to training_1/cp-0012.ckpt\n",
            "39/39 [==============================] - 34s 897ms/step - loss: 1.6105 - categorical_accuracy: 0.5887 - val_loss: 1.7559 - val_categorical_accuracy: 0.5332 - lr: 5.0000e-04\n",
            "Epoch 13/25\n",
            "39/39 [==============================] - ETA: 0s - loss: 1.5341 - categorical_accuracy: 0.6183\n",
            "Epoch 13: val_loss improved from 1.75586 to 1.71545, saving model to training_1/cp-0013.ckpt\n",
            "39/39 [==============================] - 34s 876ms/step - loss: 1.5341 - categorical_accuracy: 0.6183 - val_loss: 1.7154 - val_categorical_accuracy: 0.5213 - lr: 5.0000e-04\n",
            "Epoch 14/25\n",
            "39/39 [==============================] - ETA: 0s - loss: 1.4876 - categorical_accuracy: 0.6360\n",
            "Epoch 14: val_loss improved from 1.71545 to 1.69454, saving model to training_1/cp-0014.ckpt\n",
            "39/39 [==============================] - 34s 884ms/step - loss: 1.4876 - categorical_accuracy: 0.6360 - val_loss: 1.6945 - val_categorical_accuracy: 0.5261 - lr: 5.0000e-04\n",
            "Epoch 15/25\n",
            "39/39 [==============================] - ETA: 0s - loss: 1.4504 - categorical_accuracy: 0.6302\n",
            "Epoch 15: val_loss improved from 1.69454 to 1.64200, saving model to training_1/cp-0015.ckpt\n",
            "39/39 [==============================] - 33s 855ms/step - loss: 1.4504 - categorical_accuracy: 0.6302 - val_loss: 1.6420 - val_categorical_accuracy: 0.5521 - lr: 5.0000e-04\n",
            "Epoch 16/25\n",
            "39/39 [==============================] - ETA: 0s - loss: 1.3872 - categorical_accuracy: 0.6467\n",
            "Epoch 16: val_loss improved from 1.64200 to 1.64018, saving model to training_1/cp-0016.ckpt\n",
            "39/39 [==============================] - 34s 875ms/step - loss: 1.3872 - categorical_accuracy: 0.6467 - val_loss: 1.6402 - val_categorical_accuracy: 0.5355 - lr: 5.0000e-04\n",
            "Epoch 17/25\n",
            "39/39 [==============================] - ETA: 0s - loss: 1.3836 - categorical_accuracy: 0.6389\n",
            "Epoch 17: val_loss improved from 1.64018 to 1.58040, saving model to training_1/cp-0017.ckpt\n",
            "39/39 [==============================] - 35s 887ms/step - loss: 1.3836 - categorical_accuracy: 0.6389 - val_loss: 1.5804 - val_categorical_accuracy: 0.5640 - lr: 5.0000e-04\n",
            "Epoch 18/25\n",
            "39/39 [==============================] - ETA: 0s - loss: 1.3231 - categorical_accuracy: 0.6528\n",
            "Epoch 18: val_loss did not improve from 1.58040\n",
            "39/39 [==============================] - 34s 876ms/step - loss: 1.3231 - categorical_accuracy: 0.6528 - val_loss: 1.5922 - val_categorical_accuracy: 0.5498 - lr: 5.0000e-04\n",
            "Epoch 19/25\n",
            "39/39 [==============================] - ETA: 0s - loss: 1.2984 - categorical_accuracy: 0.6746\n",
            "Epoch 19: val_loss improved from 1.58040 to 1.56971, saving model to training_1/cp-0019.ckpt\n",
            "39/39 [==============================] - 34s 879ms/step - loss: 1.2984 - categorical_accuracy: 0.6746 - val_loss: 1.5697 - val_categorical_accuracy: 0.5498 - lr: 5.0000e-04\n",
            "Epoch 20/25\n",
            "39/39 [==============================] - ETA: 0s - loss: 1.2616 - categorical_accuracy: 0.6783\n",
            "Epoch 20: val_loss improved from 1.56971 to 1.53060, saving model to training_1/cp-0020.ckpt\n",
            "39/39 [==============================] - 34s 879ms/step - loss: 1.2616 - categorical_accuracy: 0.6783 - val_loss: 1.5306 - val_categorical_accuracy: 0.5711 - lr: 5.0000e-04\n",
            "Epoch 21/25\n",
            "39/39 [==============================] - ETA: 0s - loss: 1.2342 - categorical_accuracy: 0.6758\n",
            "Epoch 21: val_loss improved from 1.53060 to 1.50597, saving model to training_1/cp-0021.ckpt\n",
            "39/39 [==============================] - 34s 882ms/step - loss: 1.2342 - categorical_accuracy: 0.6758 - val_loss: 1.5060 - val_categorical_accuracy: 0.5758 - lr: 5.0000e-04\n",
            "Epoch 22/25\n",
            "39/39 [==============================] - ETA: 0s - loss: 1.2003 - categorical_accuracy: 0.6894\n",
            "Epoch 22: val_loss improved from 1.50597 to 1.49663, saving model to training_1/cp-0022.ckpt\n",
            "39/39 [==============================] - 34s 861ms/step - loss: 1.2003 - categorical_accuracy: 0.6894 - val_loss: 1.4966 - val_categorical_accuracy: 0.5806 - lr: 5.0000e-04\n",
            "Epoch 23/25\n",
            "39/39 [==============================] - ETA: 0s - loss: 1.1921 - categorical_accuracy: 0.6873\n",
            "Epoch 23: val_loss improved from 1.49663 to 1.47764, saving model to training_1/cp-0023.ckpt\n",
            "39/39 [==============================] - 34s 875ms/step - loss: 1.1921 - categorical_accuracy: 0.6873 - val_loss: 1.4776 - val_categorical_accuracy: 0.5877 - lr: 5.0000e-04\n",
            "Epoch 24/25\n",
            "39/39 [==============================] - ETA: 0s - loss: 1.1887 - categorical_accuracy: 0.6816\n",
            "Epoch 24: val_loss improved from 1.47764 to 1.46220, saving model to training_1/cp-0024.ckpt\n",
            "39/39 [==============================] - 34s 880ms/step - loss: 1.1887 - categorical_accuracy: 0.6816 - val_loss: 1.4622 - val_categorical_accuracy: 0.5972 - lr: 5.0000e-04\n",
            "Epoch 25/25\n",
            "39/39 [==============================] - ETA: 0s - loss: 1.1588 - categorical_accuracy: 0.6915\n",
            "Epoch 25: val_loss improved from 1.46220 to 1.44388, saving model to training_1/cp-0025.ckpt\n",
            "39/39 [==============================] - 34s 866ms/step - loss: 1.1588 - categorical_accuracy: 0.6915 - val_loss: 1.4439 - val_categorical_accuracy: 0.5900 - lr: 5.0000e-04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epoch=25\n",
        "last = os.path.join('training_1',f'cp-{epoch:04d}.ckpt')\n",
        "last"
      ],
      "metadata": {
        "id": "shZlJUQeDPpR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f8e92f91-f459-4b98-99be-a89c0e7dd75d"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'training_1/cp-0025.ckpt'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_2 = get_uncompiled_model(n_classes,model_name='mobileV1_finetune',fine_tune=4)\n",
        "    \n",
        "model_2.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_yqtSAwSqgd",
        "outputId": "b6d68dd9-9511-4f49-95b5-02a3f47f1234"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
            "                                                                 \n",
            " mobilenet_1.00_224 (Functio  (None, 1024)             3228864   \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dense (Dense)               (None, 87)                89175     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,318,039\n",
            "Trainable params: 1,139,799\n",
            "Non-trainable params: 2,178,240\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#start from checkpoint epoch 10\n",
        "\n",
        "model_2.load_weights(last)\n",
        "\n",
        "fine_tune_lr = base_learning_rate\n",
        "\n",
        "model_2.compile(optimizer=tf.keras.optimizers.legacy.SGD(learning_rate=fine_tune_lr,momentum=0.9),\n",
        "              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "              metrics = [tf.keras.metrics.CategoricalAccuracy()])"
      ],
      "metadata": {
        "id": "UTzJX1r3P-Q0"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r training_2\n",
        "\n",
        "!mkdir training_2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DzKXGP72seou",
        "outputId": "32341124-c807-4d2b-d443-a1c7953a566a"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove 'training_2': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def scheduler(epoch, lr):\n",
        "    return lr\n",
        "\n",
        "lr_callback = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
        "\n",
        "checkpoint_path = \"training_2/cp-{epoch:04d}.ckpt\"\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "N_EPOCHS=12\n",
        "\n",
        "# Create a callback that saves the model's weights\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
        "                                                 save_weights_only=True,\n",
        "                                                 save_best_only=True,\n",
        "                                                 monitor='val_categorical_accuracy',\n",
        "                                                 verbose=1,\n",
        "                                                 save_freq='epoch')\n",
        "\n",
        "history_2 = model_2.fit(\n",
        "            train_generator,\n",
        "            epochs=N_EPOCHS,\n",
        "            validation_data=val_generator,\n",
        "            verbose=1,\n",
        "            callbacks=[cp_callback,lr_callback]\n",
        "            )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JAY5IGlKQbeo",
        "outputId": "fe281a1a-d82c-4c24-fd65-1da884e97d14"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/12\n",
            "39/39 [==============================] - ETA: 0s - loss: 1.0890 - categorical_accuracy: 0.6878\n",
            "Epoch 1: val_categorical_accuracy improved from -inf to 0.57583, saving model to training_2/cp-0001.ckpt\n",
            "39/39 [==============================] - 38s 910ms/step - loss: 1.0890 - categorical_accuracy: 0.6878 - val_loss: 1.4376 - val_categorical_accuracy: 0.5758 - lr: 5.0000e-04\n",
            "Epoch 2/12\n",
            "39/39 [==============================] - ETA: 0s - loss: 1.0516 - categorical_accuracy: 0.6915\n",
            "Epoch 2: val_categorical_accuracy did not improve from 0.57583\n",
            "39/39 [==============================] - 34s 884ms/step - loss: 1.0516 - categorical_accuracy: 0.6915 - val_loss: 1.6976 - val_categorical_accuracy: 0.5403 - lr: 5.0000e-04\n",
            "Epoch 3/12\n",
            "39/39 [==============================] - ETA: 0s - loss: 1.3235 - categorical_accuracy: 0.6306\n",
            "Epoch 3: val_categorical_accuracy improved from 0.57583 to 0.58531, saving model to training_2/cp-0003.ckpt\n",
            "39/39 [==============================] - 35s 912ms/step - loss: 1.3235 - categorical_accuracy: 0.6306 - val_loss: 1.5194 - val_categorical_accuracy: 0.5853 - lr: 5.0000e-04\n",
            "Epoch 4/12\n",
            "39/39 [==============================] - ETA: 0s - loss: 1.1669 - categorical_accuracy: 0.6590\n",
            "Epoch 4: val_categorical_accuracy did not improve from 0.58531\n",
            "39/39 [==============================] - 33s 848ms/step - loss: 1.1669 - categorical_accuracy: 0.6590 - val_loss: 1.4900 - val_categorical_accuracy: 0.5829 - lr: 5.0000e-04\n",
            "Epoch 5/12\n",
            "39/39 [==============================] - ETA: 0s - loss: 1.2250 - categorical_accuracy: 0.6352\n",
            "Epoch 5: val_categorical_accuracy did not improve from 0.58531\n",
            "39/39 [==============================] - 34s 873ms/step - loss: 1.2250 - categorical_accuracy: 0.6352 - val_loss: 1.4585 - val_categorical_accuracy: 0.5758 - lr: 5.0000e-04\n",
            "Epoch 6/12\n",
            "39/39 [==============================] - ETA: 0s - loss: 1.0775 - categorical_accuracy: 0.6812\n",
            "Epoch 6: val_categorical_accuracy did not improve from 0.58531\n",
            "39/39 [==============================] - 34s 879ms/step - loss: 1.0775 - categorical_accuracy: 0.6812 - val_loss: 1.5122 - val_categorical_accuracy: 0.5853 - lr: 5.0000e-04\n",
            "Epoch 7/12\n",
            "39/39 [==============================] - ETA: 0s - loss: 1.0719 - categorical_accuracy: 0.6853\n",
            "Epoch 7: val_categorical_accuracy improved from 0.58531 to 0.59479, saving model to training_2/cp-0007.ckpt\n",
            "39/39 [==============================] - 33s 851ms/step - loss: 1.0719 - categorical_accuracy: 0.6853 - val_loss: 1.4536 - val_categorical_accuracy: 0.5948 - lr: 5.0000e-04\n",
            "Epoch 8/12\n",
            "38/39 [============================>.] - ETA: 0s - loss: 0.9951 - categorical_accuracy: 0.7044\n",
            "Epoch 8: val_categorical_accuracy did not improve from 0.59479\n",
            "39/39 [==============================] - 34s 870ms/step - loss: 0.9956 - categorical_accuracy: 0.7042 - val_loss: 1.4484 - val_categorical_accuracy: 0.5900 - lr: 5.0000e-04\n",
            "Epoch 9/12\n",
            "39/39 [==============================] - ETA: 0s - loss: 1.0497 - categorical_accuracy: 0.6919\n",
            "Epoch 9: val_categorical_accuracy did not improve from 0.59479\n",
            "39/39 [==============================] - 34s 896ms/step - loss: 1.0497 - categorical_accuracy: 0.6919 - val_loss: 1.4774 - val_categorical_accuracy: 0.5782 - lr: 5.0000e-04\n",
            "Epoch 10/12\n",
            "39/39 [==============================] - ETA: 0s - loss: 1.0696 - categorical_accuracy: 0.6882\n",
            "Epoch 10: val_categorical_accuracy did not improve from 0.59479\n",
            "39/39 [==============================] - 34s 871ms/step - loss: 1.0696 - categorical_accuracy: 0.6882 - val_loss: 1.4761 - val_categorical_accuracy: 0.5829 - lr: 5.0000e-04\n",
            "Epoch 11/12\n",
            "39/39 [==============================] - ETA: 0s - loss: 1.0782 - categorical_accuracy: 0.6779\n",
            "Epoch 11: val_categorical_accuracy improved from 0.59479 to 0.59716, saving model to training_2/cp-0011.ckpt\n",
            "39/39 [==============================] - 34s 861ms/step - loss: 1.0782 - categorical_accuracy: 0.6779 - val_loss: 1.5929 - val_categorical_accuracy: 0.5972 - lr: 5.0000e-04\n",
            "Epoch 12/12\n",
            "39/39 [==============================] - ETA: 0s - loss: 1.0629 - categorical_accuracy: 0.6779\n",
            "Epoch 12: val_categorical_accuracy did not improve from 0.59716\n",
            "39/39 [==============================] - 34s 862ms/step - loss: 1.0629 - categorical_accuracy: 0.6779 - val_loss: 1.4302 - val_categorical_accuracy: 0.5829 - lr: 5.0000e-04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UBPRsezmgYWc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}