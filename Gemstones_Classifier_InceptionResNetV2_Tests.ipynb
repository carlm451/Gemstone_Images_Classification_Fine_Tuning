{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP6t8k/wERl1pqHPdCzrmov",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/carlm451/Gemstone_Images_Classification_Fine_Tuning/blob/main/Gemstones_Classifier_InceptionResNetV2_Tests.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "ztziw_8J_Vva"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from random import shuffle\n",
        "\n",
        "from tensorflow.keras.applications.inception_resnet_v2 import InceptionResNetV2"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download/Prepare Kaggle Gemstones Dataset\n",
        "\n",
        "need to have an API key from kaggle, follow instructions here https://www.kaggle.com/docs/api"
      ],
      "metadata": {
        "id": "5MaKbAEI_3QJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "NK-3nuGlHu3q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83875687-8b32-4cb0-878d-aa7c87ad8114"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading gemstones-images.zip to /content\n",
            " 85% 47.0M/55.2M [00:00<00:00, 78.9MB/s]\n",
            "100% 55.2M/55.2M [00:00<00:00, 71.1MB/s]\n",
            "gemstones-images.zip  sample_data  test  train\n"
          ]
        }
      ],
      "source": [
        "# pulling gemstones data from kaggle\n",
        "#!pip install kaggle\n",
        "\n",
        "!mkdir ~/.kaggle\n",
        "\n",
        "#need a kaggle API key kaggle.json\n",
        "# I just upload it from my pc to /content colab directory \n",
        "!mv kaggle.json ~/.kaggle\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "#download the images to local colab drive\n",
        "!kaggle datasets download lsind18/gemstones-images\n",
        "\n",
        "# unzip the images\n",
        "!unzip gemstones-images.zip &> /dev/null  #suppress terminal output when unzipping images\n",
        "\n",
        "#expect to zee gemstones-images.zip, and test and train directories \n",
        "!ls "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "data_dir = '/content'\n",
        "\n",
        "train_dir = os.path.join(data_dir,'train')\n",
        "\n",
        "!mkdir val # going to use some training data for validation, save test data for final model evaluation \n",
        "val_dir = os.path.join(data_dir,'val')\n",
        "\n",
        "test_dir = os.path.join(data_dir,'test')\n",
        "\n",
        "def count_img_samples(directory):\n",
        "    \n",
        "    count = 0\n",
        "    \n",
        "    for i,gem_type in enumerate(os.listdir(directory)):\n",
        "        \n",
        "        gem_dir = os.path.join(directory,gem_type)\n",
        "    \n",
        "        img_list = os.listdir(gem_dir)\n",
        "\n",
        "        #print(f' dir {gem_dir} has {len(img_list)} images')\n",
        "\n",
        "        count += len(img_list)\n",
        "    \n",
        "    return count\n",
        "\n",
        "n_train = count_img_samples(train_dir)\n",
        "n_test = count_img_samples(test_dir)\n",
        "n_val = count_img_samples(val_dir)\n",
        "\n",
        "print(f'{n_train=}, {n_val=}, {n_test=}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pp-hFLr7JSjf",
        "outputId": "4f8de867-0d3d-4f01-db09-ad025f5e55e4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n_train=2856, n_val=0, n_test=363\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from random import shuffle\n",
        "\n",
        "def partition_val_data(train_dir,val_dir,val_split=0.1):\n",
        "    \n",
        "    for gem_type in os.listdir(train_dir):\n",
        "        \n",
        "        train_gem_dir = os.path.join(train_dir,gem_type)\n",
        "        \n",
        "        img_list = os.listdir(train_gem_dir)\n",
        "        \n",
        "        shuffle(img_list)\n",
        "        \n",
        "        n_samples = round(len(img_list)*val_split)\n",
        "        \n",
        "        val_img_list = img_list[:n_samples] # take n_samples random images to move\n",
        "        \n",
        "        val_gem_dir = os.path.join(val_dir,gem_type)\n",
        "        \n",
        "        if not os.path.exists(val_gem_dir):\n",
        "            \n",
        "            os.mkdir(val_gem_dir)\n",
        "            \n",
        "            for gem_img in val_img_list:\n",
        "                \n",
        "                original_path = os.path.join(train_gem_dir,gem_img)\n",
        "                \n",
        "                destination_path = os.path.join(val_gem_dir,gem_img)\n",
        "                \n",
        "                os.rename(original_path,destination_path)\n",
        "        \n",
        "            #print(f'Moved {len(os.listdir(val_gem_dir))} training images from to {val_gem_dir}')\n",
        "            \n",
        "        else:\n",
        "            \n",
        "            pass\n",
        "            #print(f'Val dir {val_gem_dir} has {len(os.listdir(val_gem_dir))} images')"
      ],
      "metadata": {
        "id": "Fq25nvVbJl24"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_split=0.15  # move 15% train to use for validation\n",
        "\n",
        "partition_val_data(train_dir,val_dir,val_split)\n",
        "\n",
        "n_train = count_img_samples(train_dir)\n",
        "n_test = count_img_samples(test_dir)\n",
        "n_val = count_img_samples(val_dir)\n",
        "\n",
        "print(f'{n_train=}, {n_val=}, {n_test=}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQcGtSnWJ8Ve",
        "outputId": "9f207806-fc34-4c25-b15d-a33b0611c5b5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n_train=2434, n_val=422, n_test=363\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls val\n",
        "\n",
        "gem_types_list = os.listdir(val_dir)\n",
        "\n",
        "n_classes = len(gem_types_list)\n",
        "\n",
        "print(f'{n_classes} classes of gemstone')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rylLl31YCc54",
        "outputId": "15f7ba3a-bca4-497b-9444-82b049b1b0a4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Alexandrite\t      Chrysocolla     Larimar\t\t 'Sapphire Blue'\n",
            " Almandine\t      Chrysoprase     Malachite\t\t 'Sapphire Pink'\n",
            " Amazonite\t      Citrine\t      Moonstone\t\t 'Sapphire Purple'\n",
            " Amber\t\t      Coral\t      Morganite\t\t 'Sapphire Yellow'\n",
            " Amethyst\t      Danburite      'Onyx Black'\t  Scapolite\n",
            " Ametrine\t      Diamond\t     'Onyx Green'\t  Serpentine\n",
            " Andalusite\t      Diaspore\t     'Onyx Red'\t\t  Sodalite\n",
            " Andradite\t      Dumortierite    Opal\t\t  Spessartite\n",
            " Aquamarine\t      Emerald\t      Pearl\t\t  Sphene\n",
            "'Aventurine Green'    Fluorite\t      Peridot\t\t  Spinel\n",
            "'Aventurine Yellow'  'Garnet Red'     Prehnite\t\t  Spodumene\n",
            " Benitoite\t      Goshenite       Pyrite\t\t  Sunstone\n",
            "'Beryl Golden'\t      Grossular       Pyrope\t\t  Tanzanite\n",
            " Bixbite\t      Hessonite      'Quartz Beer'\t 'Tigers Eye'\n",
            " Bloodstone\t      Hiddenite      'Quartz Lemon'\t  Topaz\n",
            "'Blue Lace Agate'     Iolite\t     'Quartz Rose'\t  Tourmaline\n",
            " Carnelian\t      Jade\t     'Quartz Rutilated'   Tsavorite\n",
            "'Cats Eye'\t      Jasper\t     'Quartz Smoky'\t  Turquoise\n",
            " Chalcedony\t      Kunzite\t      Rhodochrosite\t  Variscite\n",
            "'Chalcedony Blue'     Kyanite\t      Rhodolite\t\t  Zircon\n",
            "'Chrome Diopside'     Labradorite     Rhodonite\t\t  Zoisite\n",
            " Chrysoberyl\t     'Lapis Lazuli'   Ruby\n",
            "87 classes of gemstone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# generators to stream images for training/validation\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "#from tensorflow.keras.applications.efficientnet_v2 import preprocess_input\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale = 1.0/255.,\n",
        "                                   rotation_range=90,\n",
        "                                   width_shift_range=0.4,\n",
        "                                   height_shift_range=0.4,\n",
        "                                   zoom_range=0.5,\n",
        "                                   horizontal_flip=True,\n",
        "                                   vertical_flip=True\n",
        "                                  )\n",
        "\n",
        "val_datagen  = ImageDataGenerator( rescale = 1.0/255.)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(train_dir,\n",
        "                                                    batch_size=64,\n",
        "                                                    class_mode='categorical',\n",
        "                                                    target_size=(224, 224),\n",
        "                                                    keep_aspect_ratio=False,\n",
        "                                                    classes=gem_types_list) \n",
        "\n",
        "val_generator = val_datagen.flow_from_directory(val_dir,\n",
        "                                                    batch_size=64,\n",
        "                                                    class_mode='categorical',\n",
        "                                                    target_size=(224,224),\n",
        "                                                    keep_aspect_ratio=False,\n",
        "                                                    classes=gem_types_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NPf45TDBCVFU",
        "outputId": "30a3b03a-57e7-47ef-ad75-4be078cb1af2"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2434 images belonging to 87 classes.\n",
            "Found 422 images belonging to 87 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classifier based on pretrained InceptionResNetV2 model"
      ],
      "metadata": {
        "id": "eC2CNSEaCC4C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_uncompiled_model(n_classes, model_name,fine_tune=0):\n",
        "\n",
        "    tf.keras.backend.clear_session()\n",
        "\n",
        "    pretrained = InceptionResNetV2(include_top=False,pooling='avg',input_shape=(224,224,3))\n",
        "\n",
        "    if fine_tune > 0:\n",
        "        for layer in pretrained.layers[:-fine_tune]:\n",
        "            layer.trainable = False\n",
        "    else:\n",
        "        pretrained.trainable=False #freezes all children layers \n",
        "\n",
        "    inputs=tf.keras.layers.Input(shape=(224,224,3))\n",
        "\n",
        "    x=pretrained(inputs,training=False)\n",
        "\n",
        "    x = tf.keras.layers.Dense(n_classes)(x)  # make sure to use from_logits=True in loss later on\n",
        "\n",
        "    model = tf.keras.Model(inputs=inputs,outputs=x)\n",
        "    \n",
        "    return model"
      ],
      "metadata": {
        "id": "-rgQ76g_LJXL"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_1 = get_uncompiled_model(n_classes,model_name='mobilenetV2_frozen')\n",
        "    \n",
        "model_1.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hzSPtY86Cwrz",
        "outputId": "a062c04c-4aed-4f54-9c6c-e0629f53dab8"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
            "                                                                 \n",
            " inception_resnet_v2 (Functi  (None, 1536)             54336736  \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dense (Dense)               (None, 87)                133719    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 54,470,455\n",
            "Trainable params: 133,719\n",
            "Non-trainable params: 54,336,736\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "base_learning_rate = 0.0005\n",
        "\n",
        "model_1.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=base_learning_rate),\n",
        "              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "              metrics = [tf.keras.metrics.CategoricalAccuracy()])\n",
        "\n",
        "!rm -r training_1\n",
        "\n",
        "!mkdir training_1"
      ],
      "metadata": {
        "id": "O2KClwCBC1LM"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scheduler(epoch, lr):\n",
        "    return lr\n",
        "\n",
        "lr_callback = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
        "\n",
        "checkpoint_path = \"training_1/cp-{epoch:04d}.ckpt\"\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "N_EPOCHS=25\n",
        "\n",
        "# Create a callback that saves the model's weights\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
        "                                                 save_weights_only=True,\n",
        "                                                 save_best_only=True,\n",
        "                                                 verbose=1,\n",
        "                                                 save_freq='epoch')\n",
        "\n",
        "history_1 = model_1.fit(\n",
        "            train_generator,\n",
        "            epochs=N_EPOCHS,\n",
        "            validation_data=val_generator,\n",
        "            verbose=1,\n",
        "            callbacks=[cp_callback,lr_callback]\n",
        "            )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1HCvfWZXDG5k",
        "outputId": "355059a6-9e79-4d02-9140-14480975e61d"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "39/39 [==============================] - ETA: 0s - loss: 4.3956 - categorical_accuracy: 0.0509\n",
            "Epoch 1: val_loss improved from inf to 3.81097, saving model to training_1/cp-0001.ckpt\n",
            "39/39 [==============================] - 64s 1s/step - loss: 4.3956 - categorical_accuracy: 0.0509 - val_loss: 3.8110 - val_categorical_accuracy: 0.1398 - lr: 5.0000e-04\n",
            "Epoch 2/25\n",
            "39/39 [==============================] - ETA: 0s - loss: 3.4928 - categorical_accuracy: 0.1853\n",
            "Epoch 2: val_loss improved from 3.81097 to 3.18698, saving model to training_1/cp-0002.ckpt\n",
            "39/39 [==============================] - 43s 1s/step - loss: 3.4928 - categorical_accuracy: 0.1853 - val_loss: 3.1870 - val_categorical_accuracy: 0.2346 - lr: 5.0000e-04\n",
            "Epoch 3/25\n",
            "39/39 [==============================] - ETA: 0s - loss: 3.0056 - categorical_accuracy: 0.2872\n",
            "Epoch 3: val_loss improved from 3.18698 to 2.84203, saving model to training_1/cp-0003.ckpt\n",
            "39/39 [==============================] - 44s 1s/step - loss: 3.0056 - categorical_accuracy: 0.2872 - val_loss: 2.8420 - val_categorical_accuracy: 0.3365 - lr: 5.0000e-04\n",
            "Epoch 4/25\n",
            "39/39 [==============================] - ETA: 0s - loss: 2.6915 - categorical_accuracy: 0.3406\n",
            "Epoch 4: val_loss improved from 2.84203 to 2.64343, saving model to training_1/cp-0004.ckpt\n",
            "39/39 [==============================] - 43s 1s/step - loss: 2.6915 - categorical_accuracy: 0.3406 - val_loss: 2.6434 - val_categorical_accuracy: 0.3128 - lr: 5.0000e-04\n",
            "Epoch 5/25\n",
            "39/39 [==============================] - ETA: 0s - loss: 2.4732 - categorical_accuracy: 0.3788\n",
            "Epoch 5: val_loss improved from 2.64343 to 2.47416, saving model to training_1/cp-0005.ckpt\n",
            "39/39 [==============================] - 43s 1s/step - loss: 2.4732 - categorical_accuracy: 0.3788 - val_loss: 2.4742 - val_categorical_accuracy: 0.3720 - lr: 5.0000e-04\n",
            "Epoch 6/25\n",
            "39/39 [==============================] - ETA: 0s - loss: 2.3510 - categorical_accuracy: 0.4117\n",
            "Epoch 6: val_loss improved from 2.47416 to 2.40627, saving model to training_1/cp-0006.ckpt\n",
            "39/39 [==============================] - 43s 1s/step - loss: 2.3510 - categorical_accuracy: 0.4117 - val_loss: 2.4063 - val_categorical_accuracy: 0.3720 - lr: 5.0000e-04\n",
            "Epoch 7/25\n",
            "39/39 [==============================] - ETA: 0s - loss: 2.2158 - categorical_accuracy: 0.4347\n",
            "Epoch 7: val_loss improved from 2.40627 to 2.28835, saving model to training_1/cp-0007.ckpt\n",
            "39/39 [==============================] - 42s 1s/step - loss: 2.2158 - categorical_accuracy: 0.4347 - val_loss: 2.2883 - val_categorical_accuracy: 0.3957 - lr: 5.0000e-04\n",
            "Epoch 8/25\n",
            "39/39 [==============================] - ETA: 0s - loss: 2.0975 - categorical_accuracy: 0.4577\n",
            "Epoch 8: val_loss improved from 2.28835 to 2.21444, saving model to training_1/cp-0008.ckpt\n",
            "39/39 [==============================] - 43s 1s/step - loss: 2.0975 - categorical_accuracy: 0.4577 - val_loss: 2.2144 - val_categorical_accuracy: 0.4242 - lr: 5.0000e-04\n",
            "Epoch 9/25\n",
            "39/39 [==============================] - ETA: 0s - loss: 2.0058 - categorical_accuracy: 0.4811\n",
            "Epoch 9: val_loss improved from 2.21444 to 2.15206, saving model to training_1/cp-0009.ckpt\n",
            "39/39 [==============================] - 42s 1s/step - loss: 2.0058 - categorical_accuracy: 0.4811 - val_loss: 2.1521 - val_categorical_accuracy: 0.4455 - lr: 5.0000e-04\n",
            "Epoch 10/25\n",
            "39/39 [==============================] - ETA: 0s - loss: 1.9456 - categorical_accuracy: 0.4889\n",
            "Epoch 10: val_loss did not improve from 2.15206\n",
            "39/39 [==============================] - 41s 1s/step - loss: 1.9456 - categorical_accuracy: 0.4889 - val_loss: 2.1567 - val_categorical_accuracy: 0.4218 - lr: 5.0000e-04\n",
            "Epoch 11/25\n",
            "39/39 [==============================] - ETA: 0s - loss: 1.8654 - categorical_accuracy: 0.5193\n",
            "Epoch 11: val_loss improved from 2.15206 to 2.14809, saving model to training_1/cp-0011.ckpt\n",
            "39/39 [==============================] - 48s 1s/step - loss: 1.8654 - categorical_accuracy: 0.5193 - val_loss: 2.1481 - val_categorical_accuracy: 0.4289 - lr: 5.0000e-04\n",
            "Epoch 12/25\n",
            "39/39 [==============================] - ETA: 0s - loss: 1.8175 - categorical_accuracy: 0.5283\n",
            "Epoch 12: val_loss improved from 2.14809 to 2.02111, saving model to training_1/cp-0012.ckpt\n",
            "39/39 [==============================] - 47s 1s/step - loss: 1.8175 - categorical_accuracy: 0.5283 - val_loss: 2.0211 - val_categorical_accuracy: 0.4716 - lr: 5.0000e-04\n",
            "Epoch 13/25\n",
            "39/39 [==============================] - ETA: 0s - loss: 1.7975 - categorical_accuracy: 0.5164\n",
            "Epoch 13: val_loss did not improve from 2.02111\n",
            "39/39 [==============================] - 41s 1s/step - loss: 1.7975 - categorical_accuracy: 0.5164 - val_loss: 2.0220 - val_categorical_accuracy: 0.4597 - lr: 5.0000e-04\n",
            "Epoch 14/25\n",
            "39/39 [==============================] - ETA: 0s - loss: 1.7204 - categorical_accuracy: 0.5472\n",
            "Epoch 14: val_loss improved from 2.02111 to 1.98683, saving model to training_1/cp-0014.ckpt\n",
            "39/39 [==============================] - 44s 1s/step - loss: 1.7204 - categorical_accuracy: 0.5472 - val_loss: 1.9868 - val_categorical_accuracy: 0.4692 - lr: 5.0000e-04\n",
            "Epoch 15/25\n",
            "39/39 [==============================] - ETA: 0s - loss: 1.6684 - categorical_accuracy: 0.5575\n",
            "Epoch 15: val_loss improved from 1.98683 to 1.91430, saving model to training_1/cp-0015.ckpt\n",
            "39/39 [==============================] - 45s 1s/step - loss: 1.6684 - categorical_accuracy: 0.5575 - val_loss: 1.9143 - val_categorical_accuracy: 0.4834 - lr: 5.0000e-04\n",
            "Epoch 16/25\n",
            "39/39 [==============================] - ETA: 0s - loss: 1.6656 - categorical_accuracy: 0.5571\n",
            "Epoch 16: val_loss improved from 1.91430 to 1.87053, saving model to training_1/cp-0016.ckpt\n",
            "39/39 [==============================] - 43s 1s/step - loss: 1.6656 - categorical_accuracy: 0.5571 - val_loss: 1.8705 - val_categorical_accuracy: 0.4858 - lr: 5.0000e-04\n",
            "Epoch 17/25\n",
            "39/39 [==============================] - ETA: 0s - loss: 1.6074 - categorical_accuracy: 0.5698\n",
            "Epoch 17: val_loss improved from 1.87053 to 1.85085, saving model to training_1/cp-0017.ckpt\n",
            "39/39 [==============================] - 43s 1s/step - loss: 1.6074 - categorical_accuracy: 0.5698 - val_loss: 1.8508 - val_categorical_accuracy: 0.4929 - lr: 5.0000e-04\n",
            "Epoch 18/25\n",
            "39/39 [==============================] - ETA: 0s - loss: 1.5995 - categorical_accuracy: 0.5624\n",
            "Epoch 18: val_loss did not improve from 1.85085\n",
            "39/39 [==============================] - 41s 1s/step - loss: 1.5995 - categorical_accuracy: 0.5624 - val_loss: 1.8878 - val_categorical_accuracy: 0.4976 - lr: 5.0000e-04\n",
            "Epoch 19/25\n",
            "39/39 [==============================] - ETA: 0s - loss: 1.5512 - categorical_accuracy: 0.5850\n",
            "Epoch 19: val_loss improved from 1.85085 to 1.84848, saving model to training_1/cp-0019.ckpt\n",
            "39/39 [==============================] - 48s 1s/step - loss: 1.5512 - categorical_accuracy: 0.5850 - val_loss: 1.8485 - val_categorical_accuracy: 0.5071 - lr: 5.0000e-04\n",
            "Epoch 20/25\n",
            "39/39 [==============================] - ETA: 0s - loss: 1.5457 - categorical_accuracy: 0.5842\n",
            "Epoch 20: val_loss did not improve from 1.84848\n",
            "39/39 [==============================] - 41s 1s/step - loss: 1.5457 - categorical_accuracy: 0.5842 - val_loss: 1.8878 - val_categorical_accuracy: 0.4953 - lr: 5.0000e-04\n",
            "Epoch 21/25\n",
            "39/39 [==============================] - ETA: 0s - loss: 1.4709 - categorical_accuracy: 0.6134\n",
            "Epoch 21: val_loss improved from 1.84848 to 1.82492, saving model to training_1/cp-0021.ckpt\n",
            "39/39 [==============================] - 46s 1s/step - loss: 1.4709 - categorical_accuracy: 0.6134 - val_loss: 1.8249 - val_categorical_accuracy: 0.5024 - lr: 5.0000e-04\n",
            "Epoch 22/25\n",
            "39/39 [==============================] - ETA: 0s - loss: 1.5015 - categorical_accuracy: 0.6015\n",
            "Epoch 22: val_loss improved from 1.82492 to 1.80506, saving model to training_1/cp-0022.ckpt\n",
            "39/39 [==============================] - 43s 1s/step - loss: 1.5015 - categorical_accuracy: 0.6015 - val_loss: 1.8051 - val_categorical_accuracy: 0.5095 - lr: 5.0000e-04\n",
            "Epoch 23/25\n",
            "39/39 [==============================] - ETA: 0s - loss: 1.4549 - categorical_accuracy: 0.6126\n",
            "Epoch 23: val_loss improved from 1.80506 to 1.80322, saving model to training_1/cp-0023.ckpt\n",
            "39/39 [==============================] - 43s 1s/step - loss: 1.4549 - categorical_accuracy: 0.6126 - val_loss: 1.8032 - val_categorical_accuracy: 0.5142 - lr: 5.0000e-04\n",
            "Epoch 24/25\n",
            "39/39 [==============================] - ETA: 0s - loss: 1.4235 - categorical_accuracy: 0.6187\n",
            "Epoch 24: val_loss did not improve from 1.80322\n",
            "39/39 [==============================] - 41s 1s/step - loss: 1.4235 - categorical_accuracy: 0.6187 - val_loss: 1.8204 - val_categorical_accuracy: 0.4929 - lr: 5.0000e-04\n",
            "Epoch 25/25\n",
            "39/39 [==============================] - ETA: 0s - loss: 1.4731 - categorical_accuracy: 0.6060\n",
            "Epoch 25: val_loss did not improve from 1.80322\n",
            "39/39 [==============================] - 41s 1s/step - loss: 1.4731 - categorical_accuracy: 0.6060 - val_loss: 1.8177 - val_categorical_accuracy: 0.5071 - lr: 5.0000e-04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epoch=23\n",
        "last = os.path.join('training_1',f'cp-{epoch:04d}.ckpt')\n",
        "last"
      ],
      "metadata": {
        "id": "shZlJUQeDPpR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "42d3713b-2a5f-4bb4-a080-f4f7c4bab7b5"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'training_1/cp-0023.ckpt'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_2 = get_uncompiled_model(n_classes,model_name='mobilenetV2_finetune',fine_tune=4)\n",
        "    \n",
        "model_2.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_yqtSAwSqgd",
        "outputId": "02a13809-8d6e-4156-bc27-36f56b9287c7"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
            "                                                                 \n",
            " inception_resnet_v2 (Functi  (None, 1536)             54336736  \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dense (Dense)               (None, 87)                133719    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 54,470,455\n",
            "Trainable params: 3,330,135\n",
            "Non-trainable params: 51,140,320\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#start from checkpoint epoch 10\n",
        "\n",
        "model_2.load_weights(last)\n",
        "\n",
        "fine_tune_lr = base_learning_rate\n",
        "\n",
        "model_2.compile(optimizer=tf.keras.optimizers.legacy.SGD(learning_rate=fine_tune_lr,momentum=0.9),\n",
        "              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "              metrics = [tf.keras.metrics.CategoricalAccuracy()])"
      ],
      "metadata": {
        "id": "UTzJX1r3P-Q0"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r training_2\n",
        "\n",
        "!mkdir training_2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DzKXGP72seou",
        "outputId": "54d3e227-6c50-44c6-8fa4-8f8c172c7e9c"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove 'training_2': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def scheduler(epoch, lr):\n",
        "    return lr\n",
        "\n",
        "lr_callback = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
        "\n",
        "checkpoint_path = \"training_2/cp-{epoch:04d}.ckpt\"\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "N_EPOCHS=8\n",
        "\n",
        "# Create a callback that saves the model's weights\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
        "                                                 save_weights_only=True,\n",
        "                                                 save_best_only=True,\n",
        "                                                 monitor='val_categorical_accuracy',\n",
        "                                                 verbose=1,\n",
        "                                                 save_freq='epoch')\n",
        "\n",
        "history_2 = model_2.fit(\n",
        "            train_generator,\n",
        "            epochs=N_EPOCHS,\n",
        "            validation_data=val_generator,\n",
        "            verbose=1,\n",
        "            callbacks=[cp_callback,lr_callback]\n",
        "            )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JAY5IGlKQbeo",
        "outputId": "0e438fd9-d0fd-40ba-c5a3-91ac4e3b9b82"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/8\n",
            "39/39 [==============================] - ETA: 0s - loss: 1.3761 - categorical_accuracy: 0.6282\n",
            "Epoch 1: val_categorical_accuracy improved from -inf to 0.53791, saving model to training_2/cp-0001.ckpt\n",
            "39/39 [==============================] - 65s 1s/step - loss: 1.3761 - categorical_accuracy: 0.6282 - val_loss: 1.7153 - val_categorical_accuracy: 0.5379 - lr: 5.0000e-04\n",
            "Epoch 2/8\n",
            "39/39 [==============================] - ETA: 0s - loss: 1.3282 - categorical_accuracy: 0.6504\n",
            "Epoch 2: val_categorical_accuracy did not improve from 0.53791\n",
            "39/39 [==============================] - 41s 1s/step - loss: 1.3282 - categorical_accuracy: 0.6504 - val_loss: 1.7259 - val_categorical_accuracy: 0.5332 - lr: 5.0000e-04\n",
            "Epoch 3/8\n",
            "39/39 [==============================] - ETA: 0s - loss: 1.3030 - categorical_accuracy: 0.6413\n",
            "Epoch 3: val_categorical_accuracy did not improve from 0.53791\n",
            "39/39 [==============================] - 41s 1s/step - loss: 1.3030 - categorical_accuracy: 0.6413 - val_loss: 1.6924 - val_categorical_accuracy: 0.5261 - lr: 5.0000e-04\n",
            "Epoch 4/8\n",
            "39/39 [==============================] - ETA: 0s - loss: 1.2688 - categorical_accuracy: 0.6611\n",
            "Epoch 4: val_categorical_accuracy did not improve from 0.53791\n",
            "39/39 [==============================] - 41s 1s/step - loss: 1.2688 - categorical_accuracy: 0.6611 - val_loss: 1.6909 - val_categorical_accuracy: 0.5237 - lr: 5.0000e-04\n",
            "Epoch 5/8\n",
            "39/39 [==============================] - ETA: 0s - loss: 1.2565 - categorical_accuracy: 0.6561\n",
            "Epoch 5: val_categorical_accuracy did not improve from 0.53791\n",
            "39/39 [==============================] - 41s 1s/step - loss: 1.2565 - categorical_accuracy: 0.6561 - val_loss: 1.6569 - val_categorical_accuracy: 0.5379 - lr: 5.0000e-04\n",
            "Epoch 6/8\n",
            "39/39 [==============================] - ETA: 0s - loss: 1.2689 - categorical_accuracy: 0.6487\n",
            "Epoch 6: val_categorical_accuracy improved from 0.53791 to 0.54502, saving model to training_2/cp-0006.ckpt\n",
            "39/39 [==============================] - 46s 1s/step - loss: 1.2689 - categorical_accuracy: 0.6487 - val_loss: 1.6505 - val_categorical_accuracy: 0.5450 - lr: 5.0000e-04\n",
            "Epoch 7/8\n",
            "39/39 [==============================] - ETA: 0s - loss: 1.2272 - categorical_accuracy: 0.6684\n",
            "Epoch 7: val_categorical_accuracy did not improve from 0.54502\n",
            "39/39 [==============================] - 42s 1s/step - loss: 1.2272 - categorical_accuracy: 0.6684 - val_loss: 1.6989 - val_categorical_accuracy: 0.5166 - lr: 5.0000e-04\n",
            "Epoch 8/8\n",
            "39/39 [==============================] - ETA: 0s - loss: 1.2613 - categorical_accuracy: 0.6500\n",
            "Epoch 8: val_categorical_accuracy did not improve from 0.54502\n",
            "39/39 [==============================] - 41s 1s/step - loss: 1.2613 - categorical_accuracy: 0.6500 - val_loss: 1.6446 - val_categorical_accuracy: 0.5427 - lr: 5.0000e-04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UBPRsezmgYWc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}