{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPOBuTH+n+YOpf3qkkBTePd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/carlm451/Gemstone_Images_Classification_Fine_Tuning/blob/main/Gemstones_Classifier_MobileNet_Tests.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ztziw_8J_Vva"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from random import shuffle\n",
        "\n",
        "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download/Prepare Kaggle Gemstones Dataset\n",
        "\n",
        "need to have an API key from kaggle, follow instructions here https://www.kaggle.com/docs/api"
      ],
      "metadata": {
        "id": "5MaKbAEI_3QJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "NK-3nuGlHu3q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4ada5f2-e3fe-4ea7-dd9e-ee193324660b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading gemstones-images.zip to /content\n",
            " 81% 45.0M/55.2M [00:00<00:00, 157MB/s]\n",
            "100% 55.2M/55.2M [00:00<00:00, 159MB/s]\n",
            "gemstones-images.zip  sample_data  test  train\n"
          ]
        }
      ],
      "source": [
        "# pulling gemstones data from kaggle\n",
        "#!pip install kaggle\n",
        "\n",
        "!mkdir ~/.kaggle\n",
        "\n",
        "#need a kaggle API key kaggle.json\n",
        "# I just upload it from my pc to /content colab directory \n",
        "!mv kaggle.json ~/.kaggle\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "#download the images to local colab drive\n",
        "!kaggle datasets download lsind18/gemstones-images\n",
        "\n",
        "# unzip the images\n",
        "!unzip gemstones-images.zip &> /dev/null  #suppress terminal output when unzipping images\n",
        "\n",
        "#expect to zee gemstones-images.zip, and test and train directories \n",
        "!ls "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "data_dir = '/content'\n",
        "\n",
        "train_dir = os.path.join(data_dir,'train')\n",
        "\n",
        "!mkdir val # going to use some training data for validation, save test data for final model evaluation \n",
        "val_dir = os.path.join(data_dir,'val')\n",
        "\n",
        "test_dir = os.path.join(data_dir,'test')\n",
        "\n",
        "def count_img_samples(directory):\n",
        "    \n",
        "    count = 0\n",
        "    \n",
        "    for i,gem_type in enumerate(os.listdir(directory)):\n",
        "        \n",
        "        gem_dir = os.path.join(directory,gem_type)\n",
        "    \n",
        "        img_list = os.listdir(gem_dir)\n",
        "\n",
        "        #print(f' dir {gem_dir} has {len(img_list)} images')\n",
        "\n",
        "        count += len(img_list)\n",
        "    \n",
        "    return count\n",
        "\n",
        "n_train = count_img_samples(train_dir)\n",
        "n_test = count_img_samples(test_dir)\n",
        "n_val = count_img_samples(val_dir)\n",
        "\n",
        "print(f'{n_train=}, {n_val=}, {n_test=}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pp-hFLr7JSjf",
        "outputId": "3ad2753d-d374-4b52-cc5e-2108288d32ac"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n_train=2856, n_val=0, n_test=363\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-2q6K3g4A70S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from random import shuffle\n",
        "\n",
        "def partition_val_data(train_dir,val_dir,val_split=0.1):\n",
        "    \n",
        "    for gem_type in os.listdir(train_dir):\n",
        "        \n",
        "        train_gem_dir = os.path.join(train_dir,gem_type)\n",
        "        \n",
        "        img_list = os.listdir(train_gem_dir)\n",
        "        \n",
        "        shuffle(img_list)\n",
        "        \n",
        "        n_samples = round(len(img_list)*val_split)\n",
        "        \n",
        "        val_img_list = img_list[:n_samples] # take n_samples random images to move\n",
        "        \n",
        "        val_gem_dir = os.path.join(val_dir,gem_type)\n",
        "        \n",
        "        if not os.path.exists(val_gem_dir):\n",
        "            \n",
        "            os.mkdir(val_gem_dir)\n",
        "            \n",
        "            for gem_img in val_img_list:\n",
        "                \n",
        "                original_path = os.path.join(train_gem_dir,gem_img)\n",
        "                \n",
        "                destination_path = os.path.join(val_gem_dir,gem_img)\n",
        "                \n",
        "                os.rename(original_path,destination_path)\n",
        "        \n",
        "            #print(f'Moved {len(os.listdir(val_gem_dir))} training images from to {val_gem_dir}')\n",
        "            \n",
        "        else:\n",
        "            \n",
        "            pass\n",
        "            #print(f'Val dir {val_gem_dir} has {len(os.listdir(val_gem_dir))} images')"
      ],
      "metadata": {
        "id": "Fq25nvVbJl24"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_split=0.15  # move 15% train to use for validation\n",
        "\n",
        "partition_val_data(train_dir,val_dir,val_split)\n",
        "\n",
        "n_train = count_img_samples(train_dir)\n",
        "n_test = count_img_samples(test_dir)\n",
        "n_val = count_img_samples(val_dir)\n",
        "\n",
        "print(f'{n_train=}, {n_val=}, {n_test=}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQcGtSnWJ8Ve",
        "outputId": "6ca8032a-98d4-4a0b-f52c-c08e22f2d046"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n_train=2434, n_val=422, n_test=363\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls val\n",
        "\n",
        "gem_types_list = os.listdir(val_dir)\n",
        "\n",
        "n_classes = len(gem_types_list)\n",
        "\n",
        "print(f'{n_classes} classes of gemstone')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rylLl31YCc54",
        "outputId": "f8a39c27-b4f5-4537-b202-e1e99a63b93e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Alexandrite\t      Chrysocolla     Larimar\t\t 'Sapphire Blue'\n",
            " Almandine\t      Chrysoprase     Malachite\t\t 'Sapphire Pink'\n",
            " Amazonite\t      Citrine\t      Moonstone\t\t 'Sapphire Purple'\n",
            " Amber\t\t      Coral\t      Morganite\t\t 'Sapphire Yellow'\n",
            " Amethyst\t      Danburite      'Onyx Black'\t  Scapolite\n",
            " Ametrine\t      Diamond\t     'Onyx Green'\t  Serpentine\n",
            " Andalusite\t      Diaspore\t     'Onyx Red'\t\t  Sodalite\n",
            " Andradite\t      Dumortierite    Opal\t\t  Spessartite\n",
            " Aquamarine\t      Emerald\t      Pearl\t\t  Sphene\n",
            "'Aventurine Green'    Fluorite\t      Peridot\t\t  Spinel\n",
            "'Aventurine Yellow'  'Garnet Red'     Prehnite\t\t  Spodumene\n",
            " Benitoite\t      Goshenite       Pyrite\t\t  Sunstone\n",
            "'Beryl Golden'\t      Grossular       Pyrope\t\t  Tanzanite\n",
            " Bixbite\t      Hessonite      'Quartz Beer'\t 'Tigers Eye'\n",
            " Bloodstone\t      Hiddenite      'Quartz Lemon'\t  Topaz\n",
            "'Blue Lace Agate'     Iolite\t     'Quartz Rose'\t  Tourmaline\n",
            " Carnelian\t      Jade\t     'Quartz Rutilated'   Tsavorite\n",
            "'Cats Eye'\t      Jasper\t     'Quartz Smoky'\t  Turquoise\n",
            " Chalcedony\t      Kunzite\t      Rhodochrosite\t  Variscite\n",
            "'Chalcedony Blue'     Kyanite\t      Rhodolite\t\t  Zircon\n",
            "'Chrome Diopside'     Labradorite     Rhodonite\t\t  Zoisite\n",
            " Chrysoberyl\t     'Lapis Lazuli'   Ruby\n",
            "87 classes of gemstone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# generators to stream images for training/validation\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale = 1.0/255.,\n",
        "                                   rotation_range=90,\n",
        "                                   width_shift_range=0.4,\n",
        "                                   height_shift_range=0.4,\n",
        "                                   zoom_range=0.5,\n",
        "                                   horizontal_flip=True,\n",
        "                                   vertical_flip=True,\n",
        "                                  )\n",
        "\n",
        "val_datagen  = ImageDataGenerator( rescale = 1.0/255.)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(train_dir,\n",
        "                                                    batch_size=64,\n",
        "                                                    class_mode='categorical',\n",
        "                                                    target_size=(224, 224),\n",
        "                                                    keep_aspect_ratio=False,\n",
        "                                                    classes=gem_types_list) \n",
        "\n",
        "val_generator = val_datagen.flow_from_directory(val_dir,\n",
        "                                                    batch_size=64,\n",
        "                                                    class_mode='categorical',\n",
        "                                                    target_size=(224,224),\n",
        "                                                    keep_aspect_ratio=False,\n",
        "                                                    classes=gem_types_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NPf45TDBCVFU",
        "outputId": "438cd7c6-1b60-43e7-f326-3a92ac687c33"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2434 images belonging to 87 classes.\n",
            "Found 422 images belonging to 87 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classifier based on pretrained ResNet50 model"
      ],
      "metadata": {
        "id": "eC2CNSEaCC4C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_uncompiled_model(n_classes, model_name,fine_tune=0):\n",
        "\n",
        "    tf.keras.backend.clear_session()\n",
        "\n",
        "    pretrained = MobileNetV2(include_top=False,pooling='avg',input_shape=(224,224,3))\n",
        "\n",
        "    if fine_tune > 0:\n",
        "        for layer in pretrained.layers[:-fine_tune]:\n",
        "            layer.trainable = False\n",
        "    else:\n",
        "        pretrained.trainable=False #freezes all children layers \n",
        "\n",
        "    inputs=tf.keras.layers.Input(shape=(224,224,3))\n",
        "\n",
        "    x=pretrained(inputs,training=False)\n",
        "\n",
        "    x = tf.keras.layers.Dense(n_classes)(x)  # make sure to use from_logits=True in loss later on\n",
        "\n",
        "    model = tf.keras.Model(inputs=inputs,outputs=x)\n",
        "    \n",
        "    return model"
      ],
      "metadata": {
        "id": "-rgQ76g_LJXL"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_1 = get_uncompiled_model(n_classes,model_name='mobilenetV2_frozen')\n",
        "    \n",
        "model_1.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hzSPtY86Cwrz",
        "outputId": "b8a2fd3c-7be8-442c-a5e6-2d2498753805"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
            "                                                                 \n",
            " mobilenetv2_1.00_224 (Funct  (None, 1280)             2257984   \n",
            " ional)                                                          \n",
            "                                                                 \n",
            " dense (Dense)               (None, 87)                111447    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,369,431\n",
            "Trainable params: 111,447\n",
            "Non-trainable params: 2,257,984\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "base_learning_rate = 0.0005\n",
        "\n",
        "model_1.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=base_learning_rate),\n",
        "              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "              metrics = [tf.keras.metrics.CategoricalAccuracy()])"
      ],
      "metadata": {
        "id": "O2KClwCBC1LM"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scheduler(epoch, lr):\n",
        "    return lr\n",
        "\n",
        "lr_callback = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
        "\n",
        "checkpoint_path = \"training_1/cp-{epoch:04d}.ckpt\"\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "N_EPOCHS=25\n",
        "\n",
        "# Create a callback that saves the model's weights\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
        "                                                 save_weights_only=True,\n",
        "                                                 save_best_only=True,\n",
        "                                                 verbose=1,\n",
        "                                                 save_freq='epoch')\n",
        "\n",
        "history_1 = model_1.fit(\n",
        "            train_generator,\n",
        "            epochs=N_EPOCHS,\n",
        "            validation_data=val_generator,\n",
        "            verbose=1,\n",
        "            callbacks=[cp_callback,lr_callback]\n",
        "            )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1HCvfWZXDG5k",
        "outputId": "9b61daf0-e8b6-4864-990a-74a7b3537555"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "39/39 [==============================] - ETA: 0s - loss: 4.2818 - categorical_accuracy: 0.0674\n",
            "Epoch 1: val_loss improved from inf to 3.74149, saving model to training_1/cp-0001.ckpt\n",
            "39/39 [==============================] - 40s 954ms/step - loss: 4.2818 - categorical_accuracy: 0.0674 - val_loss: 3.7415 - val_categorical_accuracy: 0.1540 - lr: 5.0000e-04\n",
            "Epoch 2/25\n",
            "39/39 [==============================] - ETA: 0s - loss: 3.2743 - categorical_accuracy: 0.2551\n",
            "Epoch 2: val_loss improved from 3.74149 to 3.06195, saving model to training_1/cp-0002.ckpt\n",
            "39/39 [==============================] - 35s 882ms/step - loss: 3.2743 - categorical_accuracy: 0.2551 - val_loss: 3.0620 - val_categorical_accuracy: 0.2701 - lr: 5.0000e-04\n",
            "Epoch 3/25\n",
            "39/39 [==============================] - ETA: 0s - loss: 2.7161 - categorical_accuracy: 0.3669\n",
            "Epoch 3: val_loss improved from 3.06195 to 2.65503, saving model to training_1/cp-0003.ckpt\n",
            "39/39 [==============================] - 35s 908ms/step - loss: 2.7161 - categorical_accuracy: 0.3669 - val_loss: 2.6550 - val_categorical_accuracy: 0.3602 - lr: 5.0000e-04\n",
            "Epoch 4/25\n",
            "39/39 [==============================] - ETA: 0s - loss: 2.3405 - categorical_accuracy: 0.4540\n",
            "Epoch 4: val_loss improved from 2.65503 to 2.38065, saving model to training_1/cp-0004.ckpt\n",
            "39/39 [==============================] - 35s 890ms/step - loss: 2.3405 - categorical_accuracy: 0.4540 - val_loss: 2.3806 - val_categorical_accuracy: 0.4265 - lr: 5.0000e-04\n",
            "Epoch 5/25\n",
            "39/39 [==============================] - ETA: 0s - loss: 2.0928 - categorical_accuracy: 0.5062\n",
            "Epoch 5: val_loss improved from 2.38065 to 2.21315, saving model to training_1/cp-0005.ckpt\n",
            "39/39 [==============================] - 35s 906ms/step - loss: 2.0928 - categorical_accuracy: 0.5062 - val_loss: 2.2132 - val_categorical_accuracy: 0.4692 - lr: 5.0000e-04\n",
            "Epoch 6/25\n",
            "39/39 [==============================] - ETA: 0s - loss: 1.9270 - categorical_accuracy: 0.5415\n",
            "Epoch 6: val_loss improved from 2.21315 to 2.05191, saving model to training_1/cp-0006.ckpt\n",
            "39/39 [==============================] - 35s 901ms/step - loss: 1.9270 - categorical_accuracy: 0.5415 - val_loss: 2.0519 - val_categorical_accuracy: 0.4787 - lr: 5.0000e-04\n",
            "Epoch 7/25\n",
            "39/39 [==============================] - ETA: 0s - loss: 1.7641 - categorical_accuracy: 0.5772\n",
            "Epoch 7: val_loss improved from 2.05191 to 1.96178, saving model to training_1/cp-0007.ckpt\n",
            "39/39 [==============================] - 35s 897ms/step - loss: 1.7641 - categorical_accuracy: 0.5772 - val_loss: 1.9618 - val_categorical_accuracy: 0.5047 - lr: 5.0000e-04\n",
            "Epoch 8/25\n",
            "39/39 [==============================] - ETA: 0s - loss: 1.6717 - categorical_accuracy: 0.5826\n",
            "Epoch 8: val_loss improved from 1.96178 to 1.86620, saving model to training_1/cp-0008.ckpt\n",
            "39/39 [==============================] - 34s 877ms/step - loss: 1.6717 - categorical_accuracy: 0.5826 - val_loss: 1.8662 - val_categorical_accuracy: 0.5284 - lr: 5.0000e-04\n",
            "Epoch 9/25\n",
            "39/39 [==============================] - ETA: 0s - loss: 1.5832 - categorical_accuracy: 0.5965\n",
            "Epoch 9: val_loss improved from 1.86620 to 1.78927, saving model to training_1/cp-0009.ckpt\n",
            "39/39 [==============================] - 35s 904ms/step - loss: 1.5832 - categorical_accuracy: 0.5965 - val_loss: 1.7893 - val_categorical_accuracy: 0.5521 - lr: 5.0000e-04\n",
            "Epoch 10/25\n",
            "39/39 [==============================] - ETA: 0s - loss: 1.4973 - categorical_accuracy: 0.6343\n",
            "Epoch 10: val_loss improved from 1.78927 to 1.72100, saving model to training_1/cp-0010.ckpt\n",
            "39/39 [==============================] - 35s 907ms/step - loss: 1.4973 - categorical_accuracy: 0.6343 - val_loss: 1.7210 - val_categorical_accuracy: 0.5853 - lr: 5.0000e-04\n",
            "Epoch 11/25\n",
            "39/39 [==============================] - ETA: 0s - loss: 1.4217 - categorical_accuracy: 0.6442\n",
            "Epoch 11: val_loss improved from 1.72100 to 1.69119, saving model to training_1/cp-0011.ckpt\n",
            "39/39 [==============================] - 35s 895ms/step - loss: 1.4217 - categorical_accuracy: 0.6442 - val_loss: 1.6912 - val_categorical_accuracy: 0.5664 - lr: 5.0000e-04\n",
            "Epoch 12/25\n",
            "39/39 [==============================] - ETA: 0s - loss: 1.4064 - categorical_accuracy: 0.6463\n",
            "Epoch 12: val_loss improved from 1.69119 to 1.64954, saving model to training_1/cp-0012.ckpt\n",
            "39/39 [==============================] - 35s 878ms/step - loss: 1.4064 - categorical_accuracy: 0.6463 - val_loss: 1.6495 - val_categorical_accuracy: 0.5687 - lr: 5.0000e-04\n",
            "Epoch 13/25\n",
            "39/39 [==============================] - ETA: 0s - loss: 1.3218 - categorical_accuracy: 0.6627\n",
            "Epoch 13: val_loss improved from 1.64954 to 1.61295, saving model to training_1/cp-0013.ckpt\n",
            "39/39 [==============================] - 35s 896ms/step - loss: 1.3218 - categorical_accuracy: 0.6627 - val_loss: 1.6130 - val_categorical_accuracy: 0.5995 - lr: 5.0000e-04\n",
            "Epoch 14/25\n",
            "39/39 [==============================] - ETA: 0s - loss: 1.2804 - categorical_accuracy: 0.6705\n",
            "Epoch 14: val_loss improved from 1.61295 to 1.56895, saving model to training_1/cp-0014.ckpt\n",
            "39/39 [==============================] - 35s 898ms/step - loss: 1.2804 - categorical_accuracy: 0.6705 - val_loss: 1.5690 - val_categorical_accuracy: 0.5758 - lr: 5.0000e-04\n",
            "Epoch 15/25\n",
            "39/39 [==============================] - ETA: 0s - loss: 1.2325 - categorical_accuracy: 0.6763\n",
            "Epoch 15: val_loss did not improve from 1.56895\n",
            "39/39 [==============================] - 35s 887ms/step - loss: 1.2325 - categorical_accuracy: 0.6763 - val_loss: 1.5732 - val_categorical_accuracy: 0.5877 - lr: 5.0000e-04\n",
            "Epoch 16/25\n",
            "39/39 [==============================] - ETA: 0s - loss: 1.2014 - categorical_accuracy: 0.6968\n",
            "Epoch 16: val_loss improved from 1.56895 to 1.55658, saving model to training_1/cp-0016.ckpt\n",
            "39/39 [==============================] - 35s 891ms/step - loss: 1.2014 - categorical_accuracy: 0.6968 - val_loss: 1.5566 - val_categorical_accuracy: 0.5806 - lr: 5.0000e-04\n",
            "Epoch 17/25\n",
            "39/39 [==============================] - ETA: 0s - loss: 1.1558 - categorical_accuracy: 0.6886\n",
            "Epoch 17: val_loss improved from 1.55658 to 1.51129, saving model to training_1/cp-0017.ckpt\n",
            "39/39 [==============================] - 35s 902ms/step - loss: 1.1558 - categorical_accuracy: 0.6886 - val_loss: 1.5113 - val_categorical_accuracy: 0.6066 - lr: 5.0000e-04\n",
            "Epoch 18/25\n",
            "39/39 [==============================] - ETA: 0s - loss: 1.1423 - categorical_accuracy: 0.7050\n",
            "Epoch 18: val_loss improved from 1.51129 to 1.48363, saving model to training_1/cp-0018.ckpt\n",
            "39/39 [==============================] - 36s 912ms/step - loss: 1.1423 - categorical_accuracy: 0.7050 - val_loss: 1.4836 - val_categorical_accuracy: 0.6161 - lr: 5.0000e-04\n",
            "Epoch 19/25\n",
            "39/39 [==============================] - ETA: 0s - loss: 1.1080 - categorical_accuracy: 0.6931\n",
            "Epoch 19: val_loss improved from 1.48363 to 1.45183, saving model to training_1/cp-0019.ckpt\n",
            "39/39 [==============================] - 35s 898ms/step - loss: 1.1080 - categorical_accuracy: 0.6931 - val_loss: 1.4518 - val_categorical_accuracy: 0.6161 - lr: 5.0000e-04\n",
            "Epoch 20/25\n",
            "39/39 [==============================] - ETA: 0s - loss: 1.0681 - categorical_accuracy: 0.7210\n",
            "Epoch 20: val_loss improved from 1.45183 to 1.44200, saving model to training_1/cp-0020.ckpt\n",
            "39/39 [==============================] - 35s 898ms/step - loss: 1.0681 - categorical_accuracy: 0.7210 - val_loss: 1.4420 - val_categorical_accuracy: 0.6185 - lr: 5.0000e-04\n",
            "Epoch 21/25\n",
            "39/39 [==============================] - ETA: 0s - loss: 1.0295 - categorical_accuracy: 0.7375\n",
            "Epoch 21: val_loss did not improve from 1.44200\n",
            "39/39 [==============================] - 35s 893ms/step - loss: 1.0295 - categorical_accuracy: 0.7375 - val_loss: 1.4623 - val_categorical_accuracy: 0.6090 - lr: 5.0000e-04\n",
            "Epoch 22/25\n",
            "39/39 [==============================] - ETA: 0s - loss: 1.0077 - categorical_accuracy: 0.7280\n",
            "Epoch 22: val_loss improved from 1.44200 to 1.42804, saving model to training_1/cp-0022.ckpt\n",
            "39/39 [==============================] - 35s 900ms/step - loss: 1.0077 - categorical_accuracy: 0.7280 - val_loss: 1.4280 - val_categorical_accuracy: 0.6256 - lr: 5.0000e-04\n",
            "Epoch 23/25\n",
            "39/39 [==============================] - ETA: 0s - loss: 1.0109 - categorical_accuracy: 0.7334\n",
            "Epoch 23: val_loss improved from 1.42804 to 1.39421, saving model to training_1/cp-0023.ckpt\n",
            "39/39 [==============================] - 35s 911ms/step - loss: 1.0109 - categorical_accuracy: 0.7334 - val_loss: 1.3942 - val_categorical_accuracy: 0.6327 - lr: 5.0000e-04\n",
            "Epoch 24/25\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.9834 - categorical_accuracy: 0.7457\n",
            "Epoch 24: val_loss improved from 1.39421 to 1.39417, saving model to training_1/cp-0024.ckpt\n",
            "39/39 [==============================] - 36s 917ms/step - loss: 0.9834 - categorical_accuracy: 0.7457 - val_loss: 1.3942 - val_categorical_accuracy: 0.6303 - lr: 5.0000e-04\n",
            "Epoch 25/25\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.9884 - categorical_accuracy: 0.7301\n",
            "Epoch 25: val_loss did not improve from 1.39417\n",
            "39/39 [==============================] - 35s 909ms/step - loss: 0.9884 - categorical_accuracy: 0.7301 - val_loss: 1.4143 - val_categorical_accuracy: 0.6327 - lr: 5.0000e-04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epoch=24\n",
        "last = os.path.join('training_1',f'cp-{epoch:04d}.ckpt')\n",
        "last"
      ],
      "metadata": {
        "id": "shZlJUQeDPpR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "5b598428-384f-43a5-da6a-678437f77f23"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'training_1/cp-0024.ckpt'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_2 = get_uncompiled_model(n_classes,model_name='mobilenetV2_finetune',fine_tune=4)\n",
        "    \n",
        "model_2.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_yqtSAwSqgd",
        "outputId": "0134b304-0d5c-4e35-ede7-2c12903eafef"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
            "                                                                 \n",
            " mobilenetv2_1.00_224 (Funct  (None, 1280)             2257984   \n",
            " ional)                                                          \n",
            "                                                                 \n",
            " dense (Dense)               (None, 87)                111447    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,369,431\n",
            "Trainable params: 523,607\n",
            "Non-trainable params: 1,845,824\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#start from checkpoint epoch 10\n",
        "\n",
        "model_2.load_weights(last)\n",
        "\n",
        "fine_tune_lr = base_learning_rate\n",
        "\n",
        "model_2.compile(optimizer=tf.keras.optimizers.legacy.SGD(learning_rate=fine_tune_lr,momentum=0.9),\n",
        "              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "              metrics = [tf.keras.metrics.CategoricalAccuracy()])"
      ],
      "metadata": {
        "id": "UTzJX1r3P-Q0"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scheduler(epoch, lr):\n",
        "    return lr\n",
        "\n",
        "lr_callback = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
        "\n",
        "checkpoint_path = \"training_2/cp-{epoch:04d}.ckpt\"\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "N_EPOCHS=25\n",
        "\n",
        "# Create a callback that saves the model's weights\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
        "                                                 save_weights_only=True,\n",
        "                                                 save_best_only=True,\n",
        "                                                 monitor='val_categorical_accuracy',\n",
        "                                                 verbose=1,\n",
        "                                                 save_freq='epoch')\n",
        "\n",
        "history_2 = model_2.fit(\n",
        "            train_generator,\n",
        "            epochs=N_EPOCHS,\n",
        "            validation_data=val_generator,\n",
        "            verbose=1,\n",
        "            callbacks=[cp_callback,lr_callback]\n",
        "            )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JAY5IGlKQbeo",
        "outputId": "77de42da-ab88-4021-fc7d-8699c8bcfa2f"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.9404 - categorical_accuracy: 0.7572\n",
            "Epoch 1: val_categorical_accuracy improved from -inf to 0.63507, saving model to training_2/cp-0001.ckpt\n",
            "39/39 [==============================] - 39s 941ms/step - loss: 0.9404 - categorical_accuracy: 0.7572 - val_loss: 1.3732 - val_categorical_accuracy: 0.6351 - lr: 5.0000e-04\n",
            "Epoch 2/25\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.9072 - categorical_accuracy: 0.7592\n",
            "Epoch 2: val_categorical_accuracy improved from 0.63507 to 0.63981, saving model to training_2/cp-0002.ckpt\n",
            "39/39 [==============================] - 35s 898ms/step - loss: 0.9072 - categorical_accuracy: 0.7592 - val_loss: 1.3484 - val_categorical_accuracy: 0.6398 - lr: 5.0000e-04\n",
            "Epoch 3/25\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.8810 - categorical_accuracy: 0.7560\n",
            "Epoch 3: val_categorical_accuracy improved from 0.63981 to 0.65640, saving model to training_2/cp-0003.ckpt\n",
            "39/39 [==============================] - 35s 901ms/step - loss: 0.8810 - categorical_accuracy: 0.7560 - val_loss: 1.3273 - val_categorical_accuracy: 0.6564 - lr: 5.0000e-04\n",
            "Epoch 4/25\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.8526 - categorical_accuracy: 0.7662\n",
            "Epoch 4: val_categorical_accuracy did not improve from 0.65640\n",
            "39/39 [==============================] - 35s 892ms/step - loss: 0.8526 - categorical_accuracy: 0.7662 - val_loss: 1.3092 - val_categorical_accuracy: 0.6540 - lr: 5.0000e-04\n",
            "Epoch 5/25\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.8397 - categorical_accuracy: 0.7712\n",
            "Epoch 5: val_categorical_accuracy did not improve from 0.65640\n",
            "39/39 [==============================] - 35s 903ms/step - loss: 0.8397 - categorical_accuracy: 0.7712 - val_loss: 1.3190 - val_categorical_accuracy: 0.6540 - lr: 5.0000e-04\n",
            "Epoch 6/25\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.8056 - categorical_accuracy: 0.7810\n",
            "Epoch 6: val_categorical_accuracy improved from 0.65640 to 0.66351, saving model to training_2/cp-0006.ckpt\n",
            "39/39 [==============================] - 35s 883ms/step - loss: 0.8056 - categorical_accuracy: 0.7810 - val_loss: 1.2926 - val_categorical_accuracy: 0.6635 - lr: 5.0000e-04\n",
            "Epoch 7/25\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.8193 - categorical_accuracy: 0.7749\n",
            "Epoch 7: val_categorical_accuracy improved from 0.66351 to 0.67299, saving model to training_2/cp-0007.ckpt\n",
            "39/39 [==============================] - 35s 901ms/step - loss: 0.8193 - categorical_accuracy: 0.7749 - val_loss: 1.2903 - val_categorical_accuracy: 0.6730 - lr: 5.0000e-04\n",
            "Epoch 8/25\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.7888 - categorical_accuracy: 0.7798\n",
            "Epoch 8: val_categorical_accuracy did not improve from 0.67299\n",
            "39/39 [==============================] - 35s 910ms/step - loss: 0.7888 - categorical_accuracy: 0.7798 - val_loss: 1.2814 - val_categorical_accuracy: 0.6635 - lr: 5.0000e-04\n",
            "Epoch 9/25\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.7976 - categorical_accuracy: 0.7638\n",
            "Epoch 9: val_categorical_accuracy did not improve from 0.67299\n",
            "39/39 [==============================] - 36s 915ms/step - loss: 0.7976 - categorical_accuracy: 0.7638 - val_loss: 1.2896 - val_categorical_accuracy: 0.6564 - lr: 5.0000e-04\n",
            "Epoch 10/25\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.7703 - categorical_accuracy: 0.7786\n",
            "Epoch 10: val_categorical_accuracy did not improve from 0.67299\n",
            "39/39 [==============================] - 35s 907ms/step - loss: 0.7703 - categorical_accuracy: 0.7786 - val_loss: 1.2740 - val_categorical_accuracy: 0.6588 - lr: 5.0000e-04\n",
            "Epoch 11/25\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.7266 - categorical_accuracy: 0.7876\n",
            "Epoch 11: val_categorical_accuracy did not improve from 0.67299\n",
            "39/39 [==============================] - 35s 891ms/step - loss: 0.7266 - categorical_accuracy: 0.7876 - val_loss: 1.2699 - val_categorical_accuracy: 0.6564 - lr: 5.0000e-04\n",
            "Epoch 12/25\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.7385 - categorical_accuracy: 0.7827\n",
            "Epoch 12: val_categorical_accuracy did not improve from 0.67299\n",
            "39/39 [==============================] - 34s 879ms/step - loss: 0.7385 - categorical_accuracy: 0.7827 - val_loss: 1.2637 - val_categorical_accuracy: 0.6635 - lr: 5.0000e-04\n",
            "Epoch 13/25\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.7485 - categorical_accuracy: 0.7781\n",
            "Epoch 13: val_categorical_accuracy did not improve from 0.67299\n",
            "39/39 [==============================] - 35s 896ms/step - loss: 0.7485 - categorical_accuracy: 0.7781 - val_loss: 1.2664 - val_categorical_accuracy: 0.6588 - lr: 5.0000e-04\n",
            "Epoch 14/25\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.7480 - categorical_accuracy: 0.7835\n",
            "Epoch 14: val_categorical_accuracy did not improve from 0.67299\n",
            "39/39 [==============================] - 35s 901ms/step - loss: 0.7480 - categorical_accuracy: 0.7835 - val_loss: 1.2602 - val_categorical_accuracy: 0.6611 - lr: 5.0000e-04\n",
            "Epoch 15/25\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.7151 - categorical_accuracy: 0.7901\n",
            "Epoch 15: val_categorical_accuracy did not improve from 0.67299\n",
            "39/39 [==============================] - 35s 904ms/step - loss: 0.7151 - categorical_accuracy: 0.7901 - val_loss: 1.2767 - val_categorical_accuracy: 0.6659 - lr: 5.0000e-04\n",
            "Epoch 16/25\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.7223 - categorical_accuracy: 0.7913\n",
            "Epoch 16: val_categorical_accuracy did not improve from 0.67299\n",
            "39/39 [==============================] - 35s 908ms/step - loss: 0.7223 - categorical_accuracy: 0.7913 - val_loss: 1.2451 - val_categorical_accuracy: 0.6611 - lr: 5.0000e-04\n",
            "Epoch 17/25\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.7144 - categorical_accuracy: 0.7970\n",
            "Epoch 17: val_categorical_accuracy did not improve from 0.67299\n",
            "39/39 [==============================] - 35s 894ms/step - loss: 0.7144 - categorical_accuracy: 0.7970 - val_loss: 1.2588 - val_categorical_accuracy: 0.6588 - lr: 5.0000e-04\n",
            "Epoch 18/25\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.6784 - categorical_accuracy: 0.7966\n",
            "Epoch 18: val_categorical_accuracy did not improve from 0.67299\n",
            "39/39 [==============================] - 35s 900ms/step - loss: 0.6784 - categorical_accuracy: 0.7966 - val_loss: 1.2427 - val_categorical_accuracy: 0.6588 - lr: 5.0000e-04\n",
            "Epoch 19/25\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.6968 - categorical_accuracy: 0.7921\n",
            "Epoch 19: val_categorical_accuracy did not improve from 0.67299\n",
            "39/39 [==============================] - 34s 877ms/step - loss: 0.6968 - categorical_accuracy: 0.7921 - val_loss: 1.2383 - val_categorical_accuracy: 0.6611 - lr: 5.0000e-04\n",
            "Epoch 20/25\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.6948 - categorical_accuracy: 0.8032\n",
            "Epoch 20: val_categorical_accuracy did not improve from 0.67299\n",
            "39/39 [==============================] - 35s 884ms/step - loss: 0.6948 - categorical_accuracy: 0.8032 - val_loss: 1.2356 - val_categorical_accuracy: 0.6730 - lr: 5.0000e-04\n",
            "Epoch 21/25\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.7145 - categorical_accuracy: 0.7909\n",
            "Epoch 21: val_categorical_accuracy did not improve from 0.67299\n",
            "39/39 [==============================] - 35s 886ms/step - loss: 0.7145 - categorical_accuracy: 0.7909 - val_loss: 1.2404 - val_categorical_accuracy: 0.6706 - lr: 5.0000e-04\n",
            "Epoch 22/25\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.6627 - categorical_accuracy: 0.8032\n",
            "Epoch 22: val_categorical_accuracy did not improve from 0.67299\n",
            "39/39 [==============================] - 35s 897ms/step - loss: 0.6627 - categorical_accuracy: 0.8032 - val_loss: 1.2210 - val_categorical_accuracy: 0.6730 - lr: 5.0000e-04\n",
            "Epoch 23/25\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.6626 - categorical_accuracy: 0.8016\n",
            "Epoch 23: val_categorical_accuracy improved from 0.67299 to 0.67536, saving model to training_2/cp-0023.ckpt\n",
            "39/39 [==============================] - 36s 916ms/step - loss: 0.6626 - categorical_accuracy: 0.8016 - val_loss: 1.2305 - val_categorical_accuracy: 0.6754 - lr: 5.0000e-04\n",
            "Epoch 24/25\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.6502 - categorical_accuracy: 0.8131\n",
            "Epoch 24: val_categorical_accuracy did not improve from 0.67536\n",
            "39/39 [==============================] - 36s 925ms/step - loss: 0.6502 - categorical_accuracy: 0.8131 - val_loss: 1.2264 - val_categorical_accuracy: 0.6706 - lr: 5.0000e-04\n",
            "Epoch 25/25\n",
            "39/39 [==============================] - ETA: 0s - loss: 0.6721 - categorical_accuracy: 0.7995\n",
            "Epoch 25: val_categorical_accuracy did not improve from 0.67536\n",
            "39/39 [==============================] - 36s 913ms/step - loss: 0.6721 - categorical_accuracy: 0.7995 - val_loss: 1.2480 - val_categorical_accuracy: 0.6659 - lr: 5.0000e-04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UBPRsezmgYWc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}